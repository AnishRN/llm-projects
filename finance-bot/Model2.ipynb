{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8pdMCjE2wg6"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "token_huggingface = userdata.get('HF_TOKEN')\n",
    "token_pinecone = userdata.get('PINECONE-TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d9f3ed3"
   },
   "source": [
    "# Task\n",
    "Evaluate the base flan-t5 model and the 'yakul259/fint5-financeqa-customised' model on the 'sweatSmile/FinanceQA' dataset using rouge, bleu, and exact match metrics. Additionally, estimate and compare the carbon footprint and operational costs of both models, and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5d20468"
   },
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "### Subtask:\n",
    "Install the required libraries for Hugging Face Transformers, datasets, and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3c41d58"
   },
   "source": [
    "**Reasoning**:\n",
    "Install the necessary Python libraries for the task, including transformers, datasets, evaluate, accelerate, codecarbon, matplotlib, and seaborn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "75909f91",
    "outputId": "adab5baa-63d5-43bf-8757-6184f8ec4f8f"
   },
   "outputs": [],
   "source": [
    "%pip install transformers datasets evaluate accelerate codecarbon matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "699b1eaa"
   },
   "source": [
    "## Import models\n",
    "\n",
    "### Subtask:\n",
    "Import the base flan-t5 model without downloading all weights and the customized fint5-financeqa-customised model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1645ef60"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary classes and load the tokenizers and models for both the base flan-t5 model and the customized fint5-financeqa-customised model without downloading all weights for the flan-t5 model initially.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "c12509ee708042ffaae719af28a15ebc",
      "5e49875d0b0e4a10a54999d358556db7",
      "2a7569672d404ab883134bb1df6d1f4a",
      "9cde0f20757d4d7b9cac65e2fb1aafa3",
      "4c24fd19c52d4df2842eaf213bc3d15f",
      "05b8c3f0579247afb24082e2ed4f339f",
      "16a73ccea24d46e69c0bbbda8a262dcb",
      "225d8ead1a11421981846987857b991e",
      "f8237026335b46859a9326119fc067ac",
      "6b1a79a57084450fbc497469b71115dd",
      "7d7c6fbfa9cd417ab8c7d3e7e3c7e9eb",
      "50122cfb9d14446195a22372f5977434",
      "35469003ad524f6cbbe3c8f091070cdb",
      "ec1330e8ba1c42769baa1857093bc183",
      "c9cf4fca59dd4340b0f5aba68d10ea2e",
      "bb29832d377d4ca09dd055dc1f2896f9",
      "9af192b3e6744d288d12145af92f2dbb",
      "b0712846b65c4a01ae72f5bca5e79ce8",
      "14d8f0a6e8ec49919306bfc751b46528",
      "5c3d29be7e334739bb91820a2c47c3d7",
      "f0d3d1bf74614f3d90b0c3013479c3ea",
      "af3da6350efa4af9a8dc3f98b9c86c98",
      "f9d72d61097d4b9785a562540431ea5c",
      "0b7183b93d31462888ac7045077e5cdd",
      "c6d70af0917d4d0d9254baa63e9f08f2",
      "b5d0da0d6d8b491ca9e56b86d8d6a5a2",
      "63331572e2004627ab771a00fa9d0f55",
      "42dbc19aec4b460ca0db8bcd18285e13",
      "f7367e41392742218f1e7f69d037c364",
      "76a09796a32347cdaae7ab646ac91852",
      "857e0eee95ab4ba3a217b100e2a518ff",
      "6d6fe3c0c8d54e0ebccc1430aaa66776",
      "5b2697cef6834ec88a1df3e5888a5e5e",
      "d1c59aeb46134728b89cd4d10033c78e",
      "9942f5967c074182acb22eaefcb27079",
      "6d38049dc6794840954a02def45cb106",
      "d140588f2f8845c7bd1f6b35b02e0946",
      "491643566a714ec7ae477aafd739a260",
      "ab4d6244d2d646b999bc5828b0ea7f8e",
      "c179a264394b4fe5a1cbc0d489347e47",
      "69f25d504849422598097cf09410d32d",
      "e43098ea7130425bad418c41e662795a",
      "8955d44655d8482d815188920b0852d9",
      "3c8833681dca43df88d562798a9d784d",
      "682d6c40d7f34c57920d60aa3f7d4c99",
      "061498ce9301464b88487cf6bc094256",
      "11e1faaa90bd4086a7f0b5c249fe1491",
      "e58b8e56cc2546a19274d633f609a520",
      "972b3042f3d64f2b98bcf865f5dbe543",
      "feab5185372e4c03b51b4b46f9b3e20e",
      "903790f403e2455593a6b0742c5a238e",
      "3ade11ca328248149862ed70514cf2a7",
      "a2178514391a407eb41f317c9f86fc4d",
      "3e42de1ccd9b4607b94f6c75eca3d375",
      "e87515493aaa4125a073ea3089269cd9",
      "804dabb18339474e8da17e28fa4a1957",
      "8057e674a38b4209add38475d5b08dbd",
      "379408fd98d34bd38b79335a8270bde6",
      "1dda3b7b26664c7aa4d34b191b38c057",
      "15b60609ffe94a8ab3856d349ef14101",
      "925746694df9416abcc0e6006cd0fd70",
      "d06cebcbed6943959e48142bb17537db",
      "54274bf2da5643b9b7b22139ef179513",
      "ac6ae41e2fbd42ca9708b54f6752ee97",
      "81385b832f0f421c93a1490b5ed2696e",
      "eea0a558f07a4cc8a565af53c68ef46f",
      "acf6973acf0046119e83b3894aff05fb",
      "007ece5642034b76a65d5029b2118cfd",
      "4cd14f10a2884581ab2f5d779a8903a3",
      "ca860639434a4748a50eef04686808a0",
      "bf0aa378fa5a4cf99652144bda7ed8e7",
      "42e3a031a8a542c091091c53af56ad78",
      "2bfdcbd2a4e94b759ef8b5e7e381c8a3",
      "1af310adb8a14b45bc1e0297cb8889c1",
      "39ff543978a2414882bad5948f112623",
      "026065019d134d76a3bae5382dbc952a",
      "b297abe2537342268b7e326ffa141f32"
     ]
    },
    "id": "d197e9bc",
    "outputId": "23e634f2-1042-4e07-ae57-bdb4d9c638b5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model for the base \"google/flan-t5-base\" model.\n",
    "# Use from_pretrained with local_files_only=False and a configuration flag if available to avoid full weight download.\n",
    "# Note: AutoModelForSeq2SeqLM.from_pretrained will typically download weights by default.\n",
    "# We proceed with the standard loading and will address potential large downloads if they become an issue\n",
    "# in subsequent steps, focusing now on getting the model and tokenizer objects.\n",
    "flan_t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "flan_t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "\n",
    "# Load the tokenizer and model for the \"yakul259/fint5-financeqa-customised\" model.\n",
    "fint5_tokenizer = AutoTokenizer.from_pretrained(\"yakul259/fint5-financeqa-customised\")\n",
    "fint5_model = AutoModelForSeq2SeqLM.from_pretrained(\"yakul259/fint5-financeqa-customised\")\n",
    "\n",
    "print(\"Models and tokenizers loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec618589"
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "### Subtask:\n",
    "Load the 'sweatSmile/FinanceQA' dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e45cfd3d"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary function and load the dataset as instructed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "813f5f23",
    "outputId": "3712afd4-1d33-443a-9070-aff35ae596f3"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "finance_qa_dataset = load_dataset(\"sweatSmile/FinanceQA\")\n",
    "\n",
    "print(finance_qa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "515b9adb"
   },
   "source": [
    "## Evaluate models\n",
    "\n",
    "### Subtask:\n",
    "Evaluate both models on the dataset using rouge, bleu, and exact match metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bba7e905"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the necessary evaluation metrics and select a subset of the test set for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "057cc2f4",
    "outputId": "20d09607-a9f5-401d-b41f-893ff3eba11a"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "exact_match = load(\"exact_match\")\n",
    "\n",
    "# Select a subset of the test set\n",
    "test_dataset = finance_qa_dataset[\"test\"]\n",
    "subset_size = 150  # Choose a subset size\n",
    "test_subset = test_dataset.select(range(subset_size))\n",
    "\n",
    "print(f\"Selected a subset of the test set with size: {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "028df6df"
   },
   "source": [
    "**Reasoning**:\n",
    "Iterate through the selected subset, generate predictions for each model, and store predictions and references.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "428001ed",
    "outputId": "5a60f4ba-4094-4c96-f17a-a3ded03f6c56"
   },
   "outputs": [],
   "source": [
    "flan_t5_predictions = []\n",
    "fint5_predictions = []\n",
    "references = []\n",
    "\n",
    "for example in test_subset:\n",
    "    question = example[\"QUERY\"]\n",
    "    context = example[\"CONTEXT\"]\n",
    "    reference_answer = example[\"ANSWER\"]\n",
    "\n",
    "    # Generate prediction for flan-t5\n",
    "    flan_t5_input_text = f\"question: {question} context: {context}\"\n",
    "    flan_t5_inputs = flan_t5_tokenizer(flan_t5_input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    flan_t5_outputs = flan_t5_model.generate(**flan_t5_inputs, max_new_tokens=50)\n",
    "    flan_t5_prediction = flan_t5_tokenizer.decode(flan_t5_outputs[0], skip_special_tokens=True)\n",
    "    flan_t5_predictions.append(flan_t5_prediction)\n",
    "\n",
    "    # Generate prediction for fint5\n",
    "    fint5_input_text = f\"question: {question} context: {context}\"\n",
    "    fint5_inputs = fint5_tokenizer(fint5_input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    fint5_outputs = fint5_model.generate(**fint5_inputs, max_new_tokens=50)\n",
    "    fint5_prediction = fint5_tokenizer.decode(fint5_outputs[0], skip_special_tokens=True)\n",
    "    fint5_predictions.append(fint5_prediction)\n",
    "\n",
    "    references.append(reference_answer)\n",
    "\n",
    "print(\"Generated predictions and stored references.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33110e3e"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate and print the evaluation scores for both models using the collected predictions and references.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c71d89d3",
    "outputId": "5fa2ad62-55a9-405f-e9b2-18f29cebdea6"
   },
   "outputs": [],
   "source": [
    "# Calculate Rouge scores\n",
    "rouge_flan_t5 = rouge.compute(predictions=flan_t5_predictions, references=references)\n",
    "rouge_fint5 = rouge.compute(predictions=fint5_predictions, references=references)\n",
    "\n",
    "# Calculate Bleu scores\n",
    "# Bleu expects a list of references for each prediction\n",
    "bleu_references = [[ref] for ref in references]\n",
    "bleu_flan_t5 = bleu.compute(predictions=flan_t5_predictions, references=bleu_references)\n",
    "bleu_fint5 = bleu.compute(predictions=fint5_predictions, references=bleu_references)\n",
    "\n",
    "# Calculate Exact Match scores\n",
    "em_flan_t5 = exact_match.compute(predictions=flan_t5_predictions, references=references)\n",
    "em_fint5 = exact_match.compute(predictions=fint5_predictions, references=references)\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"Flan-T5:\")\n",
    "print(f\"  Rouge: {rouge_flan_t5}\")\n",
    "print(f\"  Bleu: {bleu_flan_t5}\")\n",
    "print(f\"  Exact Match: {em_flan_t5}\")\n",
    "print(\"\\nFinT5-FinanceQA-Customised:\")\n",
    "print(f\"  Rouge: {rouge_fint5}\")\n",
    "print(f\"  Bleu: {bleu_fint5}\")\n",
    "print(f\"  Exact Match: {em_fint5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fe5c338"
   },
   "source": [
    "## Estimate carbon footprint and operational costs\n",
    "\n",
    "### Subtask:\n",
    "Estimate the carbon footprint and operational costs for each model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4e809f1"
   },
   "source": [
    "**Reasoning**:\n",
    "Initialize and start the carbon tracker for the flan-t5 model, then run a dummy inference operation to simulate usage for tracking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "efbb44dc",
    "outputId": "dd4d848e-eadd-43b7-9353-c0d4d2c540c2"
   },
   "outputs": [],
   "source": [
    "from codecarbon import CarbonTracker\n",
    "import os\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs('carbon_footprint_flan_t5', exist_ok=True)\n",
    "os.makedirs('carbon_footprint_fint5', exist_ok=True)\n",
    "\n",
    "# Initialize and start tracker for flan-t5\n",
    "tracker_flan_t5 = CarbonTracker(output_dir='carbon_footprint_flan_t5')\n",
    "tracker_flan_t5.start()\n",
    "\n",
    "# Dummy inference for flan-t5 to simulate usage\n",
    "dummy_input_text_flan_t5 = \"question: What is the capital? context: The capital is 100.\"\n",
    "dummy_inputs_flan_t5 = flan_t5_tokenizer(dummy_input_text_flan_t5, return_tensors=\"pt\")\n",
    "_ = flan_t5_model.generate(**dummy_inputs_flan_t5, max_new_tokens=1)\n",
    "\n",
    "print(\"Started tracking and ran dummy inference for flan-t5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e955c7e3"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous command failed because `CarbonTracker` could not be imported from `codecarbon`. This likely indicates an issue with the `codecarbon` installation or a change in its API. I will try importing `OfflineEmissionsTracker` instead, which is a common alternative in `codecarbon` for tracking emissions. I will then re-run the steps to initialize and start the tracker for the flan-t5 model and perform a dummy inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPOzXmGY4Q-a",
    "outputId": "9b3c0d87-85b8-468d-97a1-a39eb15fa2dc"
   },
   "outputs": [],
   "source": [
    "from codecarbon import OfflineEmissionsTracker\n",
    "import os\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs('carbon_footprint_flan_t5', exist_ok=True)\n",
    "os.makedirs('carbon_footprint_fint5', exist_ok=True)\n",
    "\n",
    "# Initialize and start tracker for flan-t5 using OfflineEmissionsTracker\n",
    "tracker_flan_t5 = OfflineEmissionsTracker(output_dir='carbon_footprint_flan_t5')\n",
    "tracker_flan_t5.start()\n",
    "\n",
    "# Dummy inference for flan-t5 to simulate usage\n",
    "dummy_input_text_flan_t5 = \"question: What is the capital? context: The capital is 100.\"\n",
    "dummy_inputs_flan_t5 = flan_t5_tokenizer(dummy_input_text_flan_t5, return_tensors=\"pt\")\n",
    "_ = flan_t5_model.generate(**dummy_inputs_flan_t5, max_new_tokens=1)\n",
    "\n",
    "print(\"Started tracking and ran dummy inference for flan-t5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3c10427"
   },
   "source": [
    "**Reasoning**:\n",
    "Now that tracking has started and a dummy inference has been run for the flan-t5 model, the next step is to stop the tracker for flan-t5. After that, I will initialize and start the tracker for the fint5 model and run a dummy inference for it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQsCgYVM4T7b",
    "outputId": "7edd8716-61fa-4b51-9dcd-a834d5277891"
   },
   "outputs": [],
   "source": [
    "# Stop tracking for flan-t5\n",
    "tracker_flan_t5.stop()\n",
    "print(\"Stopped tracking for flan-t5.\")\n",
    "\n",
    "# Initialize and start tracker for fint5\n",
    "tracker_fint5 = OfflineEmissionsTracker(output_dir='carbon_footprint_fint5')\n",
    "tracker_fint5.start()\n",
    "\n",
    "# Dummy inference for fint5 to simulate usage\n",
    "dummy_input_text_fint5 = \"question: What is the revenue? context: The revenue is 500.\"\n",
    "dummy_inputs_fint5 = fint5_tokenizer(dummy_input_text_fint5, return_tensors=\"pt\")\n",
    "_ = fint5_model.generate(**dummy_inputs_fint5, max_new_tokens=1)\n",
    "\n",
    "print(\"Started tracking and ran dummy inference for fint5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a047b2"
   },
   "source": [
    "**Reasoning**:\n",
    "Tracking has started and a dummy inference has been run for the fint5 model. The next step is to stop the tracker for fint5. After stopping both trackers, I will analyze the generated `emissions.csv` files to get an estimate of the carbon footprint and provide a qualitative comparison of operational costs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "suvn9s3R4XK1",
    "outputId": "a040db6c-a7e1-49be-fdcd-1b2614195250"
   },
   "outputs": [],
   "source": [
    "# Stop tracking for fint5\n",
    "tracker_fint5.stop()\n",
    "print(\"Stopped tracking for fint5.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Analyze flan-t5 emissions\n",
    "try:\n",
    "    df_flan_t5 = pd.read_csv('carbon_footprint_flan_t5/emissions.csv')\n",
    "    print(\"\\nFlan-T5 Carbon Footprint (estimated based on dummy run):\")\n",
    "    display(df_flan_t5[['timestamp', 'duration', 'emissions', 'energy_consumed', 'cpu_power', 'gpu_power', 'ram_power']].tail())\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nFlan-T5 emissions.csv not found. Carbon footprint data not available.\")\n",
    "\n",
    "# Analyze fint5 emissions\n",
    "try:\n",
    "    df_fint5 = pd.read_csv('carbon_footprint_fint5/emissions.csv')\n",
    "    print(\"\\nFinT5 Carbon Footprint (estimated based on dummy run):\")\n",
    "    display(df_fint5[['timestamp', 'duration', 'emissions', 'energy_consumed', 'cpu_power', 'gpu_power', 'ram_power']].tail())\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nFinT5 emissions.csv not found. Carbon footprint data not available.\")\n",
    "\n",
    "# Qualitative comparison of operational costs\n",
    "print(\"\\nQualitative Comparison of Operational Costs:\")\n",
    "print(\"Estimating operational costs accurately requires specific infrastructure details (e.g., cloud provider, instance type, usage hours), which are not available in this setup.\")\n",
    "print(\"However, based on model size, larger models generally require more computational resources (CPU, GPU, RAM) for inference.\")\n",
    "print(\"Assuming FinT5-FinanceQA-Customised is larger or requires similar resources to Flan-T5 base for fine-tuning and inference, its operational costs are likely to be comparable or potentially higher than the base Flan-T5 model when deployed at scale.\")\n",
    "print(\"The fine-tuning process for FinT5 also incurs additional computational cost compared to using a pre-trained Flan-T5 base model directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5da3f6e1"
   },
   "source": [
    "## Compare models\n",
    "\n",
    "### Subtask:\n",
    "Compare the models based on the evaluation metrics, carbon footprint, and operational costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49f9013c"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a summary comparing the evaluation metrics, carbon footprint (with caveats), and qualitative cost considerations for both models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "2b75a67b",
    "outputId": "6767203d-bc68-4560-9270-adef67c8cdc8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Create a summary table for evaluation metrics\n",
    "evaluation_summary = pd.DataFrame({\n",
    "    'Metric': ['Rouge-1', 'Rouge-2', 'Rouge-L', 'Bleu', 'Exact Match'],\n",
    "    'Flan-T5': [\n",
    "        rouge_flan_t5['rouge1'],\n",
    "        rouge_flan_t5['rouge2'],\n",
    "        rouge_flan_t5['rougeL'],\n",
    "        bleu_flan_t5['bleu'],\n",
    "        em_flan_t5['exact_match']\n",
    "    ],\n",
    "    'FinT5-FinanceQA-Customised': [\n",
    "        rouge_fint5['rouge1'],\n",
    "        rouge_fint5['rouge2'],\n",
    "        rouge_fint5['rougeL'],\n",
    "        bleu_fint5['bleu'],\n",
    "        em_fint5['exact_match']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Evaluation Metrics Summary:\")\n",
    "display(evaluation_summary)\n",
    "\n",
    "# 2. Include estimated carbon footprint (with caveats)\n",
    "print(\"\\nEstimated Carbon Footprint (based on dummy runs - NOT representative of full-scale operations):\")\n",
    "if 'df_flan_t5' in locals() and not df_flan_t5.empty:\n",
    "    flan_t5_emissions = df_flan_t5['emissions'].sum()\n",
    "    flan_t5_energy = df_flan_t5['energy_consumed'].sum()\n",
    "    flan_t5_duration = df_flan_t5['duration'].sum()\n",
    "    print(f\"Flan-T5: Emissions = {flan_t5_emissions:.6f} kg CO2eq, Energy = {flan_t5_energy:.6f} kWh, Duration = {flan_t5_duration:.2f} seconds\")\n",
    "else:\n",
    "    print(\"Flan-T5 carbon footprint data not available.\")\n",
    "\n",
    "if 'df_fint5' in locals() and not df_fint5.empty:\n",
    "    fint5_emissions = df_fint5['emissions'].sum()\n",
    "    fint5_energy = df_fint5['energy_consumed'].sum()\n",
    "    fint5_duration = df_fint5['duration'].sum()\n",
    "    print(f\"FinT5-FinanceQA-Customised: Emissions = {fint5_emissions:.6f} kg CO2eq, Energy = {fint5_energy:.6f} kWh, Duration = {fint5_duration:.2f} seconds\")\n",
    "else:\n",
    "    print(\"FinT5 carbon footprint data not available.\")\n",
    "\n",
    "print(\"\\nNote: These carbon footprint and energy consumption values are based on short dummy inference runs and do not reflect the true costs of full dataset evaluation or production usage.\")\n",
    "\n",
    "\n",
    "# 3. Summarize qualitative comparison of operational costs\n",
    "print(\"\\nQualitative Comparison of Operational Costs:\")\n",
    "print(\"- Estimating operational costs accurately requires specific infrastructure details (e.g., cloud provider, instance type, usage hours), which are not available in this setup.\")\n",
    "print(\"- However, generally, model size, fine-tuning requirements, and inference resource needs contribute to cost differences.\")\n",
    "print(\"- The FinT5-FinanceQA-Customised model, being fine-tuned, implies an initial training/fine-tuning cost not present when using a base model like Flan-T5 directly.\")\n",
    "print(\"- For inference, if the models have significantly different sizes or computational requirements, operational costs would differ. Assuming similar inference resource needs per prediction, the per-prediction cost might be comparable, but the cumulative cost for large-scale deployment depends heavily on total usage.\")\n",
    "print(\"- Fine-tuned models might offer better performance, potentially reducing the number of inferences needed or improving efficiency, which could indirectly impact costs.\")\n",
    "\n",
    "# 4. Concise summary and trade-offs\n",
    "print(\"\\nOverall Summary and Trade-offs:\")\n",
    "print(\"Based on the evaluation metrics on a subset of the FinanceQA dataset, the FinT5-FinanceQA-Customised model demonstrates superior performance across Rouge, Bleu, and Exact Match compared to the base Flan-T5 model.\")\n",
    "print(\"The estimated carbon footprint from small dummy runs suggests a slightly higher footprint for FinT5 in this specific instance, but these figures are not reliable indicators for full-scale operations.\")\n",
    "print(\"Qualitatively, FinT5 involves the additional cost of fine-tuning and potentially similar or higher inference costs depending on deployment infrastructure and scale.\")\n",
    "print(\"Trade-offs involve choosing between the higher performance of a fine-tuned model (FinT5) at potentially higher development (fine-tuning) and operational costs, versus the lower performance of a base model (Flan-T5) with lower initial setup cost and potentially lower inference costs if it's significantly smaller or more efficient.\")\n",
    "print(\"For real-world FinanceQA tasks requiring higher accuracy, the performance gain from the fine-tuned FinT5 model would likely outweigh the increased costs, provided the performance difference scales to the full dataset and production environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8d62767"
   },
   "source": [
    "## Visualize results\n",
    "\n",
    "### Subtask:\n",
    "Visualize the comparison results using graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dc5f747"
   },
   "source": [
    "**Reasoning**:\n",
    "Create bar plots to visualize the comparison results for evaluation metrics and estimated environmental impact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7307f922",
    "outputId": "06179330-5d80-4444-8fa6-17c8a97a7e46"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for Evaluation Metrics Plot\n",
    "metrics = evaluation_summary['Metric']\n",
    "flan_t5_scores = evaluation_summary['Flan-T5']\n",
    "fint5_scores = evaluation_summary['FinT5-FinanceQA-Customised']\n",
    "\n",
    "x = np.arange(len(metrics)) # the label locations\n",
    "width = 0.35 # the width of the bars\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax1.bar(x - width/2, flan_t5_scores, width, label='Flan-T5')\n",
    "rects2 = ax1.bar(x + width/2, fint5_scores, width, label='FinT5-FinanceQA-Customised')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Comparison of Evaluation Metrics for Flan-T5 and FinT5-FinanceQA-Customised')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data for Environmental Impact Plot\n",
    "environmental_metrics = ['Estimated Carbon Emissions', 'Estimated Energy Consumption']\n",
    "flan_t5_env = [flan_t5_emissions, flan_t5_energy]\n",
    "fint5_env = [fint5_emissions, fint5_energy]\n",
    "env_units = ['kg CO2eq', 'kWh']\n",
    "\n",
    "x_env = np.arange(len(environmental_metrics)) # the label locations\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "rects_env1 = ax2.bar(x_env - width/2, flan_t5_env, width, label='Flan-T5')\n",
    "rects_env2 = ax2.bar(x_env + width/2, fint5_env, width, label='FinT5-FinanceQA-Customised')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Comparison of Estimated Environmental Impact (based on dummy runs)')\n",
    "ax2.set_xticks(x_env)\n",
    "ax2.set_xticklabels(environmental_metrics)\n",
    "ax2.legend()\n",
    "\n",
    "# Add text annotations\n",
    "ax2.text(0, max(flan_t5_env[0], fint5_env[0]) * 1.1, 'Note: Estimates based on dummy runs and not representative of full-scale operations.', horizontalalignment='center', verticalalignment='bottom', color='red', fontsize=9)\n",
    "ax2.text(1, max(flan_t5_env[1], fint5_env[1]) * 1.1, 'Note: Estimates based on dummy runs and not representative of full-scale operations.', horizontalalignment='center', verticalalignment='bottom', color='red', fontsize=9)\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f1607e8"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   The `yakul259/fint5-financeqa-customised` model significantly outperformed the base Flan-T5 model across all evaluated metrics (Rouge-1, Rouge-2, Rouge-L, Bleu, and Exact Match) on a subset of 150 examples from the FinanceQA dataset.\n",
    "*   For Rouge metrics, FinT5 achieved scores of approximately Rouge-1: 0.401, Rouge-2: 0.283, Rouge-L: 0.401, while Flan-T5 scored Rouge-1: 0.318, Rouge-2: 0.202, Rouge-L: 0.316.\n",
    "*   FinT5 had a Bleu score of 0.008 compared to Flan-T5's 0.0.\n",
    "*   The Exact Match score for FinT5 was 0.04, which is higher than Flan-T5's 0.013.\n",
    "*   Based on short dummy inference runs, the estimated carbon footprint for FinT5 (approximately 0.000082 kg CO2eq and 0.000173 kWh) was slightly higher than for Flan-T5 (approximately 0.000073 kg CO2eq and 0.000153 kWh). These environmental impact figures are not representative of full-scale operations.\n",
    "*   Qualitatively, the fine-tuned FinT5 model incurs an initial fine-tuning cost not present with the base Flan-T5. Operational costs for inference depend on model size, computational requirements, deployment infrastructure, and scale, and could be similar or higher for FinT5.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   For tasks requiring high accuracy on financial QA, the fine-tuned `yakul259/fint5-financeqa-customised` model is demonstrably better than the base Flan-T5 model, based on the evaluation subset.\n",
    "*   A more comprehensive evaluation on the full test dataset and a detailed analysis of computational resource usage during full inference would provide a more accurate comparison of environmental impact and operational costs at scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "teTr35cn45NW",
    "outputId": "b4dc8f39-4116-4217-cc99-8cfa3bc1670c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for Evaluation Metrics Plot\n",
    "metrics = evaluation_summary['Metric']\n",
    "flan_t5_scores = evaluation_summary['Flan-T5']\n",
    "fint5_scores = evaluation_summary['FinT5-FinanceQA-Customised']\n",
    "\n",
    "x = np.arange(len(metrics)) # the label locations\n",
    "width = 0.35 # the width of the bars\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 4)) # Reduced figure size\n",
    "rects1 = ax1.bar(x - width/2, flan_t5_scores, width, label='Flan-T5')\n",
    "rects2 = ax1.bar(x + width/2, fint5_scores, width, label='FinT5-FinanceQA-Customised')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Comparison of Evaluation Metrics for Flan-T5 and FinT5-FinanceQA-Customised')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data for Environmental Impact Plot\n",
    "environmental_metrics = ['Estimated Carbon Emissions', 'Estimated Energy Consumption']\n",
    "flan_t5_env = [flan_t5_emissions, flan_t5_energy]\n",
    "fint5_env = [fint5_emissions, fint5_energy]\n",
    "env_units = ['kg CO2eq', 'kWh']\n",
    "\n",
    "x_env = np.arange(len(environmental_metrics)) # the label locations\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(6, 4)) # Reduced figure size\n",
    "rects_env1 = ax2.bar(x_env - width/2, flan_t5_env, width, label='Flan-T5')\n",
    "rects_env2 = ax2.bar(x_env + width/2, fint5_env, width, label='FinT5-FinanceQA-Customised')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Comparison of Estimated Environmental Impact (based on dummy runs)')\n",
    "ax2.set_xticks(x_env)\n",
    "ax2.set_xticklabels(environmental_metrics)\n",
    "ax2.legend()\n",
    "\n",
    "# Removed text annotations\n",
    "# ax2.text(0, max(flan_t5_env[0], fint5_env[0]) * 1.1, 'Note: Estimates based on dummy runs and not representative of full-scale operations.', horizontalalignment='center', verticalalignment='bottom', color='red', fontsize=9)\n",
    "# ax2.text(1, max(flan_t5_env[1], fint5_env[1]) * 1.1, 'Note: Estimates based on dummy runs and not representative of full-scale operations.', horizontalalignment='center', verticalalignment='bottom', color='red', fontsize=9)\n",
    "\n",
    "fig2.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9hCDRBd5mr8",
    "outputId": "5ec36cb5-acce-4621-af7f-9d34307b12bc"
   },
   "outputs": [],
   "source": [
    "# Save the evaluation metrics summary to a CSV file\n",
    "evaluation_summary.to_csv('evaluation_metrics_summary.csv', index=False)\n",
    "\n",
    "print(\"Evaluation metrics summary saved to evaluation_metrics_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PEqZO_-76A9Q",
    "outputId": "4d09ed0f-2636-4d24-f53c-e6fd1dceeadb"
   },
   "outputs": [],
   "source": [
    "evaluation_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vMnQ2T56CPk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
